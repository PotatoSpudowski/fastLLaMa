from enum import Enum
import sys
import multiprocessing
from typing import Callable, List, Optional, Union

sys.path.append("./build/interfaces/python")

import fastLlama

class Logger:
    def log_info(self, func_name: str, message: str) -> None:
        print(f"[Info]: Func('{func_name}') {message}", flush=True, end='')
    
    def log_err(self, func_name: str, message: str) -> None:
        print(f"[Error]: Func('{func_name}') {message}", flush=True, end='')
    
    def log_warn(self, func_name: str, message: str) -> None:
        print(f"[Warn]: Func('{func_name}') {message}", flush=True, end='')
    
    def reset(self) -> None:
        return None

class ModelKind(Enum):
    LLAMA_7B = "LLAMA-7B"
    LLAMA_13B = "LLAMA-13B"
    LLAMA_30B = "LLAMA-30B"
    LLAMA_65B = "LLAMA-65B"
    ALPACA_LORA_7B = "ALPACA-LORA-7B"
    ALPACA_LORA_13B = "ALPACA-LORA-13B"
    ALPACA_LORA_30B = "ALPACA-LORA-30B"
    ALPACA_LORA_65B = "ALPACA-LORA-65B"

class Model:
    def __init__(
        self,
        id: Union[str, ModelKind], # Model Id. See the readme for support models and get the id
        path: str, # File path to the model
        num_threads: int = multiprocessing.cpu_count(), # Number of threads to use during evaluation of model
        n_ctx: int = 512, # Size of the memory context to use
        last_n_size: int = 64, # Number of token that the model can remember
        seed: int = 0, # Random number seed to be used in model
        tokens_to_keep: int = 200, # Number of tokens to keep when tokens are removed from buffer to save memory
        n_batch: int = 16, # Size of the token batch that will be processed at a given time
        logger: Optional[Logger] = None, # Logger to be used for reporting messages
        ):
        normalized_id = id if type(id) == str else id.value
        if logger is None:
            self.inner = fastLlama.Model(
                normalized_id,
                path,
                num_threads,
                n_ctx,
                last_n_size,
                seed,
                tokens_to_keep,
                n_batch,
            )
        else:
            self.inner = fastLlama.Model(
                normalized_id,
                path,
                num_threads,
                n_ctx,
                last_n_size,
                seed,
                tokens_to_keep,
                n_batch,
                log_info=logger.log_info,
                log_err=logger.log_err,
                log_warn=logger.log_warn,
                log_reset=logger.reset
            )

    def ingest(self, prompt: str, is_system_prompt: bool = False) -> bool:
        return self.inner.ingest(prompt, is_system_prompt)
    
    def generate(
            self,
            streaming_fn=Callable[[str], None],
            num_tokens: int = 100, # Max number of tokens that will be generated by the model
            top_k: int = 40, # Controls the diversity by limiting the selection to the top k highest probability tokens
            top_p: float = .95, # Filters out tokens based on cumulative probability, further refining diversity
            temp: float = .8, # Adjusts the sampling temperature, influencing creativity and randomness
            repeat_penalty: float = 1.0, # Penalizes repeated tokens to reduce redundancy in generated text
            stop_words: List[str] = [], # Words that will stop the generate function when it encounters them inside the token buffer
        ) -> bool:
        return self.inner.generate(streaming_fn, num_tokens, top_k, top_p, temp, repeat_penalty, stop_words)
            
